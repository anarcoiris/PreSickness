# Plan de IntegraciÃ³n MiniLLM para EM Predictor

**Responsables:** Agent ML (Brain), Agent Backend (Backus)  
**Estado:** ğŸŸ¡ Esperando dataset etiquetado  
**Ãšltima actualizaciÃ³n:** 02/12/2025

---

## ğŸ“‹ Resumen

Integrar el modelo **MiniLLM (TinyGPTv2)** existente como extractor de features lingÃ¼Ã­sticas para el pipeline de predicciÃ³n de brotes de Esclerosis MÃºltiple. El objetivo es capturar patrones sutiles en el lenguaje que puedan anticipar episodios clÃ­nicos.

---

## ğŸ¯ Objetivos

1. **Extraer embeddings contextuales** de los mensajes diarios usando las capas internas de MiniLLM.
2. **Fine-tune opcional** del modelo en el corpus especÃ­fico del paciente para mejorar la representaciÃ³n.
3. **Calcular features derivadas** (perplexity, entropÃ­a, coherencia) que complementen sentiment y mÃ©tricas lÃ©xicas.
4. **Integrar con el pipeline TFT** como features adicionales de entrada.

---

## ğŸ—ï¸ Arquitectura Propuesta

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FLUJO DE DATOS                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  [Mensajes crudos]                                                  â”‚
â”‚        â”‚                                                            â”‚
â”‚        â–¼                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
â”‚  â”‚ parse_chat_     â”‚  â† Limpieza, normalizaciÃ³n, timestamps         â”‚
â”‚  â”‚ export.py       â”‚                                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
â”‚           â”‚                                                         â”‚
â”‚           â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ Feature         â”‚â”€â”€â”€â”€â–¶â”‚ MiniLLM         â”‚                        â”‚
â”‚  â”‚ Extractor       â”‚     â”‚ (TinyGPTv2)     â”‚                        â”‚
â”‚  â”‚ (existente)     â”‚     â”‚                 â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚           â”‚                       â”‚                                 â”‚
â”‚           â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â”‚           â”‚  â”‚ embeddings (n_embd=256/384)                          â”‚
â”‚           â”‚  â”‚ perplexity por mensaje                               â”‚
â”‚           â”‚  â”‚ attention entropy                                    â”‚
â”‚           â–¼  â–¼                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
â”‚  â”‚ Feature Store   â”‚  â† Postgres/Redis                              â”‚
â”‚  â”‚ (feature_       â”‚                                                â”‚
â”‚  â”‚  windows)       â”‚                                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
â”‚           â”‚                                                         â”‚
â”‚           â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
â”‚  â”‚ TFT Training /  â”‚  â† Modelo de series temporales                 â”‚
â”‚  â”‚ Inference       â”‚                                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Features a Extraer con MiniLLM

### Nivel 1: Embeddings directos
| Feature | DimensiÃ³n | DescripciÃ³n |
|---------|-----------|-------------|
| `emb_mean` | n_embd | Media de embeddings de tokens del mensaje |
| `emb_cls` | n_embd | Embedding del primer token (si se usa CLS) |
| `emb_last` | n_embd | Embedding del Ãºltimo token |

### Nivel 2: MÃ©tricas derivadas
| Feature | Tipo | DescripciÃ³n |
|---------|------|-------------|
| `perplexity` | float | QuÃ© tan "sorprendente" es el texto para el modelo |
| `attention_entropy` | float | DispersiÃ³n de la atenciÃ³n (foco vs difuso) |
| `token_repetition` | float | Ratio de tokens repetidos |
| `vocab_coverage` | float | % del vocabulario usado vs disponible |

### Nivel 3: Patrones temporales
| Feature | Tipo | DescripciÃ³n |
|---------|------|-------------|
| `perplexity_trend_7d` | float | Tendencia de perplexity Ãºltimos 7 dÃ­as |
| `emb_drift_7d` | float | Distancia coseno entre embeddings actuales y hace 7 dÃ­as |
| `coherence_drop` | float | Cambio en coherencia semÃ¡ntica |

---

## ğŸ”§ ImplementaciÃ³n TÃ©cnica

### Paso 1: Wrapper de MiniLLM para embeddings

```python
# services/feature-extractor/minillm_embeddings.py

import torch
from MiniLLM.model import TinyGPTv2

class MiniLLMEmbedder:
    """Extrae embeddings usando MiniLLM."""
    
    def __init__(self, checkpoint_path: str, device: str = "cpu"):
        self.device = device
        self.model, self.tokenizer = self._load_model(checkpoint_path)
        self.model.eval()
    
    def _load_model(self, ckpt_path: str):
        ckpt = torch.load(ckpt_path, map_location=self.device, weights_only=False)
        config = ckpt.get("model_config", {})
        
        model = TinyGPTv2(
            vocab_size=config.get("vocab_size", 32000),
            block_size=config.get("block_size", 256),
            n_embd=config.get("n_embd", 384),
            n_layer=config.get("n_layer", 8),
            n_head=config.get("n_head", 6),
            use_rope=config.get("use_rope", True),
        )
        model.load_state_dict(ckpt["model_state_dict"])
        model.to(self.device)
        
        # Cargar tokenizer
        from tokenizers import Tokenizer
        tokenizer = Tokenizer.from_file("MiniLLM/tokenizer.json")
        
        return model, tokenizer
    
    def get_embeddings(self, text: str) -> dict:
        """Extrae embeddings y mÃ©tricas de un texto."""
        ids = self.tokenizer.encode(text).ids
        if len(ids) == 0:
            return self._empty_embeddings()
        
        # Truncar a block_size
        ids = ids[:self.model.block_size]
        x = torch.tensor([ids], dtype=torch.long, device=self.device)
        
        with torch.no_grad():
            # Forward pass - obtener embeddings internos
            tok_emb = self.model.token_emb(x)
            
            # Pasar por bloques transformer
            if self.model.use_rope:
                cos, sin = self.model.rotary_emb(x.size(1), device=x.device)
                rope_cos_sin = (cos, sin)
            else:
                rope_cos_sin = None
            
            hidden = self.model.drop(tok_emb)
            for block in self.model.blocks:
                hidden = block(hidden, rope_cos_sin=rope_cos_sin)
            
            hidden = self.model.ln_f(hidden)  # (1, seq_len, n_embd)
            
            # Calcular perplexity
            logits = self.model.head(hidden)
            perplexity = self._compute_perplexity(logits, x)
        
        # Extraer features
        hidden_np = hidden[0].cpu().numpy()
        
        return {
            "emb_mean": hidden_np.mean(axis=0).tolist(),
            "emb_last": hidden_np[-1].tolist(),
            "perplexity": perplexity,
            "seq_len": len(ids),
        }
    
    def _compute_perplexity(self, logits, targets):
        """Calcula perplexity del texto."""
        # Shift para autoregressive
        shift_logits = logits[:, :-1, :].contiguous()
        shift_targets = targets[:, 1:].contiguous()
        
        loss = torch.nn.functional.cross_entropy(
            shift_logits.view(-1, shift_logits.size(-1)),
            shift_targets.view(-1),
            reduction="mean"
        )
        return float(torch.exp(loss).item())
    
    def _empty_embeddings(self):
        return {
            "emb_mean": [0.0] * self.model.n_embd,
            "emb_last": [0.0] * self.model.n_embd,
            "perplexity": 0.0,
            "seq_len": 0,
        }
```

### Paso 2: IntegraciÃ³n con Feature Extractor existente

AÃ±adir al `services/feature-extractor/worker.py`:

```python
# En FeatureExtractor.__init__
self.minillm_embedder = MiniLLMEmbedder(
    checkpoint_path="MiniLLM/runs/colmena/ckpt_best.pt",
    device=self.device
)

# Nuevo mÃ©todo
def extract_llm_features(self, text: str) -> dict:
    """Extrae features usando MiniLLM."""
    return self.minillm_embedder.get_embeddings(text)
```

### Paso 3: Actualizar schema de feature_windows

```sql
-- AÃ±adir columnas para embeddings LLM
ALTER TABLE feature_windows 
ADD COLUMN IF NOT EXISTS llm_embedding FLOAT[] DEFAULT NULL,
ADD COLUMN IF NOT EXISTS llm_perplexity FLOAT DEFAULT NULL,
ADD COLUMN IF NOT EXISTS llm_perplexity_trend FLOAT DEFAULT NULL;
```

---

## ğŸ“ PreparaciÃ³n del Dataset

### Formato esperado de entrada

```
dataset/
â”œâ”€â”€ messages/
â”‚   â”œâ”€â”€ 2024-01-01.txt   # Mensajes del dÃ­a
â”‚   â”œâ”€â”€ 2024-01-02.txt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ events.csv           # Eventos clÃ­nicos
â”‚   # date,event_type,severity,notes
â”‚   # 2024-03-15,relapse,moderate,fatiga severa
â”‚   # 2024-06-20,relapse,mild,hormigueo piernas
â””â”€â”€ metadata.json        # Info del paciente (anonimizada)
```

### Script de preparaciÃ³n (a ejecutar cuando tengas el dataset)

```bash
# 1. Parsear mensajes (si vienen de WhatsApp/Telegram)
python MiniLLM/parse_chat_export.py export.json --out dataset/messages/

# 2. Crear dataset supervisado
python scripts/prepare_ms_dataset.py \
    --messages dataset/messages/ \
    --events dataset/events.csv \
    --output dataset/prepared/ \
    --horizons 7,14,30

# 3. Extraer features con MiniLLM
python scripts/extract_llm_features.py \
    --input dataset/prepared/features.parquet \
    --checkpoint MiniLLM/runs/colmena/ckpt_best.pt \
    --output dataset/prepared/features_with_llm.parquet
```

---

## ğŸ§ª ValidaciÃ³n

### MÃ©tricas objetivo
- **CorrelaciÃ³n perplexity-brote**: Esperamos que perplexity aumente ~7-14 dÃ­as antes de un brote
- **Embedding drift**: Cambio significativo en representaciÃ³n semÃ¡ntica pre-brote
- **Feature importance**: LLM features deberÃ­an aparecer en top-10 de SHAP values

### Experimentos planificados
1. **Baseline sin LLM**: Solo features lÃ©xicas + sentiment
2. **Con embeddings MiniLLM**: AÃ±adir emb_mean como feature
3. **Con perplexity**: AÃ±adir perplexity y su tendencia
4. **Full**: Todas las features LLM

---

## â±ï¸ Timeline estimado

| Fase | DuraciÃ³n | Dependencias |
|------|----------|--------------|
| Recibir dataset etiquetado | â€” | Usuario |
| Parsear y limpiar mensajes | 1 dÃ­a | Dataset |
| Extraer features LLM | 1 dÃ­a | Paso anterior |
| Entrenar TFT con LLM features | 2-3 dÃ­as | Features |
| Evaluar y comparar | 1 dÃ­a | Modelo |

---

## ğŸ“ Notas

- El modelo `colmena` ya entrenado puede usarse directamente o hacer fine-tune en el corpus especÃ­fico
- Perplexity es especialmente interesante: un modelo "sorprendido" por el texto puede indicar patrones anÃ³malos
- Los embeddings de 384 dims pueden reducirse con PCA si es necesario para el TFT

---

## ğŸ”— Referencias

- `MiniLLM/README.md` - DocumentaciÃ³n del modelo
- `MiniLLM/model.py` - Arquitectura TinyGPTv2
- `MiniLLM/generation.py` - Funciones de perplexity
- `train_tft.py` - Pipeline de entrenamiento actual

