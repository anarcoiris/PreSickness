# Mini-LLM v2: Transformer Mejorado para Dominios Especializados

Sistema completo para entrenar modelos de lenguaje ligeros pero competentes en dominios especÃ­ficos como ciencias, historia e historiografÃ­a.

## ðŸŽ¯ CaracterÃ­sticas Principales

### Arquitectura Mejorada
- âœ… **RoPE (Rotary Position Embeddings)**: Mejor extrapolaciÃ³n a secuencias largas
- âœ… **Pre-Layer Normalization**: Mayor estabilidad de entrenamiento
- âœ… **Weight Tying**: ReducciÃ³n de parÃ¡metros sin pÃ©rdida de calidad
- âœ… **Gradient Checkpointing**: Entrena modelos mÃ¡s grandes con menos memoria
- âœ… **Flash Attention** (opcional): 2-3x mÃ¡s rÃ¡pido

### Dataset Inteligente
- âœ… **Document-Aware Sampling**: Respeta lÃ­mites de documentos
- âœ… **Overlapping Windows**: Mejor cobertura del corpus
- âœ… **AnÃ¡lisis AutomÃ¡tico**: Sugiere hiperparÃ¡metros Ã³ptimos

### Entrenamiento Robusto
- âœ… **Learning Rate Warmup + Cosine Decay**: Convergencia mÃ¡s rÃ¡pida
- âœ… **Gradient Clipping & Accumulation**: Estabilidad en GPUs pequeÃ±as
- âœ… **Mixed Precision (AMP)**: Entrena 2x mÃ¡s rÃ¡pido
- âœ… **Early Stopping**: Evita overfitting automÃ¡ticamente
- âœ… **Perplexity Tracking**: MÃ©trica interpretable

### GeneraciÃ³n Avanzada
- âœ… **Top-k, Top-p, Temperature**: Control fino de creatividad
- âœ… **Repetition Penalty**: Reduce loops
- âœ… **Streaming**: Tokens en tiempo real
- âœ… **Batch Generation**: MÃºltiples textos eficientemente

## ðŸ“¦ InstalaciÃ³n

```bash
# Clonar repositorio
git clone <tu-repo>
cd mini-llm-v2

# Instalar dependencias
pip install torch tokenizers rich tqdm ftfy

# Opcional: Flash Attention (requiere CUDA)
pip install flash-attn --no-build-isolation
```

## ðŸš€ GuÃ­a de Inicio RÃ¡pido

### 1. Preparar Corpus

```bash
# Limpia y normaliza tu corpus
python main.py prepare-corpus \
    --input datos_raw.txt \
    --output corpus_limpio.txt \
    --preserve-case  # Mantener mayÃºsculas (recomendado para ciencias)
```

### 2. Analizar Corpus

```bash
# Analiza estructura y obtÃ©n recomendaciones
python main.py analyze-corpus \
    --corpus corpus_limpio.txt \
    --separator "<|doc|>"  # Si tienes separadores de documento
```

Salida ejemplo:
```
ANÃLISIS DEL CORPUS
==================================================================
Tokens totales: 1,234,567
Documentos encontrados: 150
Longitud promedio: 8,230 tokens
Longitud mÃ­nima: 120 tokens
Longitud mÃ¡xima: 45,000 tokens

ðŸ’¡ RECOMENDACIONES:
   Block size sugerido: 512
   Stride sugerido: 256 (50% overlap)
```

### 3. Entrenar Tokenizer

```bash
python main.py init-tokenizer \
    --files corpus_limpio.txt \
    --vocab-size 32000 \
    --out tokenizer.json
```

### 4. Entrenar Modelo

#### OpciÃ³n A: ConfiguraciÃ³n Predefinida (Recomendado)

```bash
# Modelo pequeÃ±o (~10M parÃ¡metros, ideal para 50-100MB de datos)
python main.py train \
    --tokenizer tokenizer.json \
    --corpus corpus_limpio.txt \
    --outdir runs/small_model \
    --config small

# Modelo mediano (~30M parÃ¡metros, ideal para 200-500MB)
python main.py train \
    --tokenizer tokenizer.json \
    --corpus corpus_limpio.txt \
    --outdir runs/medium_model \
    --config medium

# Modelo grande (~70M parÃ¡metros, ideal para >500MB)
python main.py train \
    --tokenizer tokenizer.json \
    --corpus corpus_limpio.txt \
    --outdir runs/large_model \
    --config large
```

#### OpciÃ³n B: ConfiguraciÃ³n Custom

```bash
python main.py train \
    --tokenizer tokenizer.json \
    --corpus corpus_limpio.txt \
    --outdir runs/custom_model \
    --config custom \
    --block-size 512 \
    --n-embd 384 \
    --n-layer 10 \
    --n-head 6 \
    --batch-size 16 \
    --epochs 30 \
    --lr 5e-4 \
    --warmup-steps 1000
```

### 5. Generar Texto

```bash
# GeneraciÃ³n simple
python main.py generate \
    --tokenizer tokenizer.json \
    --ckpt runs/small_model/ckpt_best.pt \
    --prompt "La fotosÃ­ntesis es un proceso" \
    --max-tokens 200 \
    --temperature 0.8

# GeneraciÃ³n con streaming
python main.py generate \
    --tokenizer tokenizer.json \
    --ckpt runs/small_model/ckpt_best.pt \
    --prompt "Durante el Imperio Romano" \
    --max-tokens 150 \
    --stream

# MÃºltiples prompts desde archivo
python main.py generate \
    --tokenizer tokenizer.json \
    --ckpt runs/small_model/ckpt_best.pt \
    --prompt-file prompts.txt \
    --output generaciones.txt \
    --batch-generate
```

### 6. Evaluar Modelo

```bash
python main.py evaluate \
    --tokenizer tokenizer.json \
    --ckpt runs/small_model/ckpt_best.pt \
    --test-file test_corpus.txt
```

## ðŸ“Š Configuraciones Recomendadas

### Para Dataset PequeÃ±o (50-100MB)
```bash
--config small
--block-size 256
--batch-size 32
--epochs 30
--lr 5e-4
```
**Resultado esperado**: Perplexity < 50, ~10M parÃ¡metros

### Para Dataset Mediano (200-500MB)
```bash
--config medium
--block-size 512
--batch-size 16
--epochs 20
--lr 3e-4
```
**Resultado esperado**: Perplexity < 30, ~30M parÃ¡metros

### Para Dataset Grande (>500MB)
```bash
--config large
--block-size 1024
--batch-size 8
--epochs 15
--lr 2e-4
--gradient-checkpointing
```
**Resultado esperado**: Perplexity < 20, ~70M parÃ¡metros

## ðŸŽ›ï¸ ParÃ¡metros Importantes

### Arquitectura
- `--block-size`: Longitud de contexto (256/512/1024)
- `--n-embd`: DimensiÃ³n de embeddings (256/384/512)
- `--n-layer`: Profundidad del modelo (6/8/12)
- `--n-head`: NÃºmero de attention heads (debe dividir n-embd)

### Entrenamiento
- `--lr`: Learning rate (3e-4 a 5e-4 tÃ­picamente)
- `--warmup-steps`: Pasos de warmup (500-2000)
- `--grad-clip`: Gradient clipping (1.0 recomendado)
- `--accumulation-steps`: Para simular batch size mayor
- `--patience`: Early stopping (3-5 Ã©pocas)

### GeneraciÃ³n
- `--temperature`: Creatividad (0.7=conservador, 1.0=balanceado, 1.5=creativo)
- `--top-k`: Limita a k tokens mÃ¡s probables (50 tÃ­pico)
- `--top-p`: Nucleus sampling (0.9 recomendado)
- `--repetition-penalty`: Reduce repeticiÃ³n (1.0-1.2)

## ðŸ“ Estructura de Archivos

```
mini-llm-v2/
â”œâ”€â”€ model.py           # Arquitectura del transformer
â”œâ”€â”€ dataset.py         # Dataset con document-awareness
â”œâ”€â”€ training.py        # Training loop robusto
â”œâ”€â”€ generation.py      # GeneraciÃ³n de texto
â”œâ”€â”€ main.py           # CLI principal
â”œâ”€â”€ standarize.py        # Limpieza de texto (legacy)
â””â”€â”€ runs/                # Checkpoints y logs
    â””â”€â”€ exp1/
        â”œâ”€â”€ ckpt_best.pt     # Mejor modelo
        â”œâ”€â”€ ckpt_last.pt     # Ãšltimo checkpoint
        â”œâ”€â”€ history.json     # MÃ©tricas de entrenamiento
        â””â”€â”€ config.json      # ConfiguraciÃ³n usada
```

## ðŸ”§ SoluciÃ³n de Problemas

### "CUDA out of memory"
```bash
# SoluciÃ³n 1: Reduce batch size
--batch-size 8

# SoluciÃ³n 2: Usa gradient accumulation
--batch-size 8 --accumulation-steps 4  # Simula batch=32

# SoluciÃ³n 3: Activa gradient checkpointing
--gradient-checkpointing

# SoluciÃ³n 4: Reduce tamaÃ±o del modelo
--n-embd 256 --n-layer 6
```

### "Perplexity muy alto (>100)"
- âœ… Entrena mÃ¡s Ã©pocas
- âœ… Aumenta `--warmup-steps`
- âœ… Reduce learning rate
- âœ… Aumenta tamaÃ±o del modelo
- âœ… Verifica calidad del corpus

### "Modelo repite texto"
```bash
# Durante generaciÃ³n, usa:
--repetition-penalty 1.2
--top-p 0.9
--temperature 0.8
```

### "Entrenamiento muy lento"
- âœ… Activa mixed precision: (por defecto activo)
- âœ… Reduce `--eval-every` (menos evaluaciones)
- âœ… Usa GPU con `--device cuda`
- âœ… Aumenta `--num-workers` (DataLoader)

## ðŸ“ˆ MÃ©tricas de Ã‰xito

### MÃ­nimo Viable
- âœ… Perplexity < 50 en validaciÃ³n
- âœ… Genera 3+ oraciones coherentes
- âœ… No loops infinitos

### Bueno
- âœ… Perplexity < 30
- âœ… Mantiene contexto por 1-2 pÃ¡rrafos
- âœ… GramÃ¡tica mayormente correcta

###